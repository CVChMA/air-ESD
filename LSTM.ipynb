{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6847516d-6533-4b41-985f-3d8a030a1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "from itertools import product  \n",
    "\n",
    "# Set a fixed random seed to ensure reproducibility\n",
    "torch.manual_seed(seed=1)                       \n",
    "torch.cuda.manual_seed(seed=1)                  \n",
    "np.random.seed(seed=1)                         \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Use GPU to accelerate computation\n",
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de9d7c5f-91f1-4d14-9042-7660c7bac042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('LSTMDataset.csv',header=None)\n",
    "X = df.drop(columns=[0, 1]).values\n",
    "y = df[[0, 1]].values.astype(\"float32\")\n",
    "\n",
    "# Data processing\n",
    "# 1-Feature normalization\n",
    "X_mean = X.mean(axis=0).reshape(1000,1)\n",
    "X_std = X.std(axis=0).reshape(1000,1)\n",
    "for i in range(X.shape[1]):\n",
    "    X[:, i] = (X[:, i] - X_mean[i]) / X_std[i]\n",
    "X_normalized = (X - X_mean.T) / X_std.T   \n",
    "# 2-Remove outliers \n",
    "indices_to_remove = np.any(np.abs(X_normalized) > 3, axis=1)   \n",
    "indices_to_remove = np.where(indices_to_remove)[0]  \n",
    "X = np.delete(X, indices_to_remove, axis=0)  \n",
    "y = np.delete(y, indices_to_remove, axis=0)\n",
    "# 3-Label normalization \n",
    "y_max = y.max(axis=0)\n",
    "y[:,0] = y[:,0] / y_max[0]\n",
    "y[:,1] = y[:,1] / y_max[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "653ac579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset splitting: 90% training set, 5% validation set, 5% test set\n",
    "X_train, X_nontrain, Y_train, Y_nontrain = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_nontrain, Y_nontrain, test_size=0.5, random_state=42)\n",
    "X_train = X_train.T\n",
    "X_val = X_val.T\n",
    "X_test = X_test.T\n",
    "Y_train = Y_train.T\n",
    "Y_val = Y_val.T\n",
    "Y_test = Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f58233ac-152d-4ad5-8893-3c24eed6d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate sequence data with 25 time steps\n",
    "n_x = 400\n",
    "(T_x, m) = X_train.shape   \n",
    "reshaped_X_train = np.zeros((m, int(13*(T_x//n_x)-1), n_x))\n",
    "(T_x, m) = X_val.shape   \n",
    "reshaped_X_val = np.zeros((m, int(13*(T_x//n_x)-1), n_x))\n",
    "(T_x, m) = X_test.shape   \n",
    "reshaped_X_test = np.zeros((m, int(13*(T_x//n_x)-1), n_x))\n",
    "for i in range(int(13*(T_x//n_x)-1)):\n",
    "    for j in range(n_x):\n",
    "        reshaped_X_train[:, i, j] = X_train[int(n_x/16*i+j), :]\n",
    "        reshaped_X_val[:, i, j] = X_val[int(n_x/16*i+j), :]\n",
    "        reshaped_X_test[:, i, j] = X_test[int(n_x/16*i+j), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f03bdc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the LSTM model  \n",
    "class LSTMModel(nn.Module):  \n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_coeff):  \n",
    "        super(LSTMModel, self).__init__()  \n",
    "        self.hidden_size = hidden_size  \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)  \n",
    "        self.fc = nn.Sequential(  \n",
    "            nn.Linear(hidden_size, output_size),  \n",
    "            nn.Sigmoid()  \n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_coeff)\n",
    "        \n",
    "    def forward(self, x):  \n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device) # hidden state  \n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device) # cell state\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])                                  # last step  \n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4487590f-8a0c-4972-8891-93ed13d4ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_predictions(test_predictions, Y_test, y_max):\n",
    "\n",
    "    # recover label\n",
    "    test_predictions_recovered = np.zeros_like(test_predictions)\n",
    "    Y_test_recovered = np.zeros_like(Y_test)\n",
    "\n",
    "    test_predictions_recovered[0] = test_predictions[0] * y_max[0]\n",
    "    test_predictions_recovered[1] = test_predictions[1] * y_max[1]\n",
    "    Y_test_recovered[0] = Y_test[0] * y_max[0]\n",
    "    Y_test_recovered[1] = Y_test[1] * y_max[1]\n",
    "\n",
    "    return test_predictions_recovered, Y_test_recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6386f4ee-8f16-4ae5-935b-5b1d586a710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage loss\n",
    "def criterion(y_pred, y):\n",
    "    Loss = torch.mean(torch.div(torch.abs(torch.sub(y_pred, y)), y) * 100) \n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78547b9d-ad63-4dc8-aa33-cf7bc95ef868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train  \n",
    "def train_lstm_model(X_train, Y_train, X_val, Y_val, BATCH_SIZE, input_size, hidden_size, output_size,  \n",
    "                      learning_rate, iterations, dropout_coeff):\n",
    "    \n",
    "    # Set hyperparameters\n",
    "    model = LSTMModel(input_size, hidden_size, output_size, dropout_coeff)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "    \n",
    "    # Use GPU to accelerate computation\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)  \n",
    "    Y_train = torch.tensor(Y_train, dtype=torch.float32).to(device)  \n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)  \n",
    "    Y_val = torch.tensor(Y_val, dtype=torch.float32).to(device)\n",
    "    torch_dataset = Data.TensorDataset(X_train, Y_train.T)\n",
    "    loader = Data.DataLoader(\n",
    "        dataset = torch_dataset,     \n",
    "        batch_size = BATCH_SIZE,     \n",
    "        shuffle = False,             \n",
    "        num_workers = 12,             \n",
    "    )\n",
    "\n",
    "    # training  \n",
    "    errors = []  \n",
    "    for iteration in range(iterations):  \n",
    "        model.train()  \n",
    "        optimizer.zero_grad()    \n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, Y_train.T) \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        \n",
    "    # evaluating \n",
    "    with torch.no_grad():  \n",
    "        model.eval()  \n",
    "        outputs = model(X_val)\n",
    "        val_loss = criterion(outputs, Y_val.T) \n",
    "           \n",
    "    return model, val_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c32e91c0-ec92-4c8b-a79a-cb679a691e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter: {'hidden_size': 512, 'learning_rate': 0.00015, 'dropout_coeff': 0.35}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter range \n",
    "input_size =  400                                 # Feature dimension per time step\n",
    "hidden_sizes = [128, 256, 512]                    # Dimension of LSTM hidden state                    \n",
    "output_size = 2                                   # Dimension of Labels  \n",
    "learning_rates = [0.0001, 0.000095, 0.00015] \n",
    "iterations = 15000\n",
    "batch_size = 512\n",
    "dropout_coeffs = [0.25, 0.3, 0.35]\n",
    "\n",
    "# Find the optimal hyperparameters  \n",
    "best_val_loss = float('inf')        \n",
    "best_params = None                 \n",
    "best_model = None                  \n",
    "\n",
    "for hidden_size, learning_rate, dropout_coeff in product(hidden_sizes, learning_rates, dropout_coeffs):  \n",
    "    \n",
    "    BATCH_SIZE = batch_size  \n",
    "    model, val_loss = train_lstm_model(reshaped_X_train, Y_train, reshaped_X_val, Y_val, batch_size, \n",
    "                                         input_size, hidden_size, output_size,  \n",
    "                                          learning_rate, iterations, dropout_coeff)  \n",
    "    if val_loss < best_val_loss:  \n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "        best_params = {  \n",
    "            'hidden_size': hidden_size,  \n",
    "            'learning_rate': learning_rate,  \n",
    "            'dropout_coeff': dropout_coeff,  \n",
    "        }         \n",
    "\n",
    "print(\"Best hyperparameter:\", best_params) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c00572d-3429-4f94-814d-c32ee12048f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage error of inductance on the test set: 0.947745680809021%\n",
      "Percentage error of arc length on the test set: 0.8981080651283264%\n",
      "Average Percentage error on the test set: 0.9229269027709961%\n",
      "Comparison:\n",
      " Voltage    Predict-L Actual-L   Predict-Arc Actual-Arc\n",
      "[[12000.     2252.718  2250.        0.648     0.64 ]\n",
      " [16000.     4995.825  5000.        0.458     0.45 ]\n",
      " [16000.     4990.314  5000.        6.891     6.9  ]\n",
      " ...\n",
      " [ 4000.     3368.116  3250.        4.477     4.5  ]\n",
      " [ 4000.     4112.193  4000.        4.272     4.3  ]\n",
      " [12000.     3981.267  4000.        0.185     0.19 ]]\n",
      "Indices and error values of the top 10 samples with the largest errors:\n",
      "Index: 78, \tError rate: 4.241%\n",
      "Index: 21, \tError rate: 4.542%\n",
      "Index: 199, \tError rate: 5.115%\n",
      "Index: 106, \tError rate: 5.600%\n",
      "Index: 116, \tError rate: 6.171%\n",
      "Index: 172, \tError rate: 7.372%\n",
      "Index: 79, \tError rate: 12.207%\n",
      "Index: 112, \tError rate: 17.097%\n",
      "Index: 257, \tError rate: 17.515%\n",
      "Index: 213, \tError rate: 19.864%\n",
      "\n",
      "Predicted and actual values of the top 10 samples with the largest errors:\n",
      " Voltage    Predict-L Actual-L   Predict-Arc Actual-Arc\n",
      "6000\t4278\t4250\t0.111\t0.120\n",
      "4000\t2738\t3000\t6.221\t6.200\n",
      "4000\t4520\t4500\t0.108\t0.120\n",
      "8000\t2027\t2000\t0.108\t0.120\n",
      "4000\t4187\t4750\t6.431\t6.400\n",
      "4000\t3001\t3500\t6.431\t6.400\n",
      "4000\t6051\t8000\t6.596\t6.600\n",
      "4000\t3653\t5500\t6.857\t6.900\n",
      "4000\t3768\t5750\t6.861\t6.900\n",
      "4000\t2757\t2000\t7.064\t7.200\n"
     ]
    }
   ],
   "source": [
    "# Model performance on the test set\n",
    "\n",
    "reshaped_X_test = torch.tensor(reshaped_X_test, dtype=torch.float32).to(device)\n",
    "with torch.no_grad():  \n",
    "    best_model.eval()    \n",
    "    outputs = best_model(reshaped_X_test)\n",
    "test_predictions = outputs.cpu().T\n",
    "test_predictions_recovered, Y_test_recovered = recover_predictions(test_predictions, Y_test, y_max)\n",
    "test_loss = np.mean(abs(test_predictions_recovered - Y_test_recovered) * 100 / Y_test_recovered, axis=1)\n",
    "\n",
    "print(f'Percentage error of inductance on the test set: {test_loss[0]}%')\n",
    "print(f'Percentage error of arc length on the test set: {test_loss[1]}%')\n",
    "print(f'Average Percentage error on the test set: {(test_loss[0]+test_loss[1])/2}%')\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# Compare with labels\n",
    "print(\"Comparison:\")\n",
    "print(\" Voltage    Predict-L Actual-L   Predict-Arc Actual-Arc\")\n",
    "comparison = np.concatenate((\n",
    "    (X_test[0, :] * X_std[0] + X_mean[0]).T.reshape(-1, 1),  \n",
    "    test_predictions_recovered[0].reshape(-1, 1),      \n",
    "    Y_test_recovered[0].reshape(-1, 1),                \n",
    "    test_predictions_recovered[1].reshape(-1, 1),      \n",
    "    Y_test_recovered[1].reshape(-1, 1)                 \n",
    "), axis=1)\n",
    "print(comparison)\n",
    "\n",
    "test_loss = np.mean(abs(test_predictions_recovered - Y_test_recovered) * 100 / Y_test_recovered, axis=0)\n",
    "top_10_indices = np.argsort(test_loss)[-10:]  \n",
    "\n",
    "print(\"Indices and error values of the top 10 samples with the largest errors:\")  \n",
    "for idx in top_10_indices:  \n",
    "    print(f'Index: {idx}, \\tError rate: {test_loss[idx]:.3f}%') \n",
    "  \n",
    "print(\"\\nPredicted and actual values of the top 10 samples with the largest errors:\")  \n",
    "print(\" Voltage    Predict-L Actual-L   Predict-Arc Actual-Arc\")  \n",
    "\n",
    "for idx in top_10_indices: \n",
    "    \n",
    "    voltage = X_test[0, idx] * X_std[0][0] + X_mean[0][0]\n",
    "\n",
    "    pred_L = test_predictions_recovered[0, idx]  \n",
    "    actual_L = Y_test_recovered[0, idx]  \n",
    "\n",
    "    pred_Arc = test_predictions_recovered[1, idx]  \n",
    "    actual_Arc = Y_test_recovered[1, idx]  \n",
    "    pred_Arc_rounded = np.round(pred_Arc, 3)  \n",
    "    actual_Arc_rounded = np.round(actual_Arc, 3)  \n",
    "\n",
    "    print(f\"{voltage:.0f}\\t{pred_L:.0f}\\t{actual_L:.0f}\\t{pred_Arc_rounded:.3f}\\t{actual_Arc_rounded:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
